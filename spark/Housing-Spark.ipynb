{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = SparkSession.builder().appName(\"Housing-Spark\").getOrCreate()\n",
    "      \n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"longitude\", FloatType, true),\n",
    "    StructField(\"latitude\", FloatType, true),\n",
    "    StructField(\"housing_median_age\", FloatType, true),\n",
    "    StructField(\"total_rooms\", FloatType, true),\n",
    "    StructField(\"total_bedrooms\", FloatType, true),\n",
    "    StructField(\"population\", FloatType, true),\n",
    "    StructField(\"households\", FloatType, true),\n",
    "    StructField(\"median_income\", FloatType, true),\n",
    "    StructField(\"median_house_value\", FloatType, true),\n",
    "    StructField(\"ocean_proximity\", StringType, true)\n",
    "    ))\n",
    "\n",
    "val housing = spark.read.\n",
    "      option(\"header\",\"true\").schema(customSchema).csv(\"../datasets/housing/housing.csv\")\n",
    "            \n",
    "housing.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|summary|          longitude|         latitude|housing_median_age|       total_rooms|    total_bedrooms|        population|       households|     median_income|median_house_value|ocean_proximity|\n",
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "|  count|              20640|            20640|             20640|             20640|             20433|             20640|            20640|             20640|             20640|          20640|\n",
      "|   mean|-119.56970444871473|35.63186143109965|28.639486434108527|2635.7630813953488| 537.8705525375618|1425.4767441860465|499.5396802325581|3.8706710030346416|206855.81690891474|           null|\n",
      "| stddev|  2.003531742932898|2.135952380602968| 12.58555761211163|2181.6152515827944|421.38507007403115|  1132.46212176534|382.3297528316098|1.8998217183639696|115395.61587441359|           null|\n",
      "|    min|            -124.35|            32.54|               1.0|               2.0|               1.0|               3.0|              1.0|            0.4999|           14999.0|      <1H OCEAN|\n",
      "|    max|            -114.31|            41.95|              52.0|           39320.0|            6445.0|           35682.0|           6082.0|           15.0001|          500001.0|     NEAR OCEAN|\n",
      "+-------+-------------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- housing_median_age: float (nullable = true)\n",
      " |-- total_rooms: float (nullable = true)\n",
      " |-- total_bedrooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- median_income: float (nullable = true)\n",
      " |-- median_house_value: float (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|         ISLAND|    5|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.groupBy('ocean_proximity).count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random selection = 16549, 4091\n"
     ]
    }
   ],
   "source": [
    "val Array(random_train_df, random_test_df) = housing.randomSplit(Array(0.8, 0.2), seed=42L)\n",
    "println(s\"Random selection = ${random_train_df.count}, ${random_test_df.count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|income_cat|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|       5.0|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|       5.0|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|       5.0|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|       4.0|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|       3.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val housing_income_cat = housing.withColumn(\"income_cat\", when(ceil('median_income / 1.5) < 5, ceil('median_income / 1.5)).otherwise(5.0))\n",
    "housing_income_cat.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original proportion line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "+----------+-----+--------------------+\n",
      "|income_cat|count|               ratio|\n",
      "+----------+-----+--------------------+\n",
      "|       3.0| 7236|  0.3505813953488372|\n",
      "|       2.0| 6581|  0.3188468992248062|\n",
      "|       4.0| 3639| 0.17630813953488372|\n",
      "|       5.0| 2362| 0.11443798449612404|\n",
      "|       1.0|  822|0.039825581395348836|\n",
      "+----------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val total = housing_income_cat.count() \n",
    "val overall_distr =  housing_income_cat.groupBy($\"income_cat\").count().withColumn(\"ratio\", ($\"count\" / total))\n",
    "\n",
    "overall_distr.sort(desc(\"ratio\")).show()                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random selection = 16549, 4091\n"
     ]
    }
   ],
   "source": [
    "val Array(random_train_temp, random_test_temp) = housing_income_cat.randomSplit(Array(0.8, 0.2), seed=42L)\n",
    "println(s\"Random selection = ${random_train_temp.count}, ${random_test_temp.count}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Originally I thought I need to provide fractions by manually calcuating percent using original percentage * .8 Actually, Dataset.stat will figure it out for me.  I only need to provide percentage.  The followings just leave for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(5.0 -> 0.09155038759689924, 1.0 -> 0.03186046511627907, 2.0 -> 0.25507751937984496, 3.0 -> 0.2804651162790698, 4.0 -> 0.141046511627907)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val manual_fractions = overall_distr.rdd.map {\n",
    "              case Row(key: Double, c:Long, value: Double) =>\n",
    "                 key -> value * 0.8\n",
    "           }.collectAsMap().toMap\n",
    "manual_fractions           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentative Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16549"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractions = housing_income_cat.select($\"income_cat\").distinct().rdd.map {\n",
    "    case Row(key: Double) =>\n",
    "      key -> 0.8\n",
    "}.collectAsMap.toMap\n",
    "\n",
    "val strat_train_temp = housing_income_cat.stat.sampleBy(\"income_cat\", fractions, 42L)\n",
    "val sample_count = strat_train_temp.count()\n",
    "sample_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8017926356589147"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_count.toDouble / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val strat_test_temp = spark.createDataFrame(housing_income_cat.rdd.subtract(strat_train_temp.rdd), \n",
    "      housing_income_cat.schema)\n",
    "strat_train_temp.count() + strat_test_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strat_train_set = strat_train_temp.drop($\"income_cat\")\n",
    "val strat_test_set = strat_test_temp.drop($\"income_cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use function to take compare proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|income_cat|             Overall|             Random|         Stratified|        Rand. %error|       Strat. %error|\n",
      "+----------+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "|       1.0|0.039825581395348836|0.03963985739319596|0.04024412351199468| -0.4663434798583097|  1.0509378695340956|\n",
      "|       4.0| 0.17630813953488372|0.17547888089914798|0.17614357362982658|-0.47034620064813737|-0.09333993625665471|\n",
      "|       3.0|  0.3505813953488372|0.35180373436461415| 0.3515016013052148| 0.34866054844715677| 0.26247997429011605|\n",
      "|       2.0|  0.3188468992248062| 0.3177231252643664| 0.3173605655930872| -0.3524493928502892|-0.46615903599271746|\n",
      "|       5.0| 0.11443798449612404|0.11535440207867545|0.11475013595987672|  0.8007984294606842|  0.2727691029574544|\n",
      "+----------+--------------------+-------------------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def income_cat_proportions(df: Dataset[Row], col: String) = {\n",
    "  val tot = df.count  \n",
    "  df.groupBy($\"income_cat\").count().withColumn(col, ($\"count\" / tot)).drop(\"count\")   \n",
    "}\n",
    "\n",
    "val compare_props = income_cat_proportions(housing_income_cat, \"Overall\").\n",
    "      join(income_cat_proportions(random_train_temp, \"Random\"), \"income_cat\").\n",
    "      join(income_cat_proportions(strat_train_temp, \"Stratified\"), \"income_cat\").\n",
    "      withColumn(\"Rand. %error\", ($\"Random\" / $\"Overall\") * 100 - 100).\n",
    "      withColumn(\"Strat. %error\", ($\"Stratified\" / $\"Overall\") * 100 - 100)\n",
    "      \n",
    "compare_props.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- housing_median_age: float (nullable = true)\n",
      " |-- total_rooms: float (nullable = true)\n",
      " |-- total_bedrooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- median_income: float (nullable = true)\n",
      " |-- median_house_value: float (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out median using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|percentile_approx(CAST(longitude AS DOUBLE), CAST(0.5 AS DOUBLE), 10000)|\n",
      "+------------------------------------------------------------------------+\n",
      "|                                                     -118.48999786376953|\n",
      "+------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"select percentile_approx(longitude, 0.5) from df\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentative apply Imputer, StringIndex and OneHotEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark does not require that all fields must be numeric to apply Imputer or all fields must be categorical to apply StringIndex and OneHotEncoding.  Spark allows specifying input columns and output columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(Array(-118.5199966430664), Array(34.27000045776367), Array(29.0), Array(2131.0), Array(435.0), Array(1164.0), Array(410.0), Array(3.53410005569458))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val housing_value = strat_train_set.drop(\"median_house_value\")\n",
    "val housing_num = housing_value.drop(\"ocean_proximity\")\n",
    "val housing_cat = housing_value.select($\"ocean_proximity\")\n",
    "\n",
    "housing_num.stat.approxQuantile(housing_num.schema.fieldNames, Array(0.5), 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|total_bedrooms_out|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -122.17|   37.75|              38.0|      992.0|          null|     732.0|     259.0|       1.6196|             435.0|\n",
      "|  -122.28|   37.78|              29.0|     5154.0|          null|    3741.0|    1273.0|       2.5762|             435.0|\n",
      "|  -122.24|   37.75|              45.0|      891.0|          null|     384.0|     146.0|       4.9489|             435.0|\n",
      "|   -122.1|   37.69|              41.0|      746.0|          null|     387.0|     161.0|       3.9063|             435.0|\n",
      "|  -122.14|   37.67|              37.0|     3342.0|          null|    1635.0|     557.0|       4.7933|             435.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val imputer = new Imputer().setStrategy(\"median\").setInputCols(Array(\"total_bedrooms\")).setOutputCols(Array(\"total_bedrooms_out\"))\n",
    "    \n",
    "val imputerModel = imputer.fit(housing_num)\n",
    "val X = imputerModel.transform(housing_num)\n",
    "X.filter(isnull($\"total_bedrooms\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"ocean_proximity\").setOutputCol(\"op_index\")\n",
    "val indexed = indexer.fit(housing_cat).transform(housing_cat)\n",
    "val op_cat_arr = indexed.distinct().select($\"ocean_proximity\").sort(\"op_index\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder will numeric index into a Spark Vector.  Spark in default use condensed form of Spark Vector: the last category is represented by all zeros.  We override it using normal form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+-------------------+\n",
      "|ocean_proximity|op_index|ocean_proximity_out|\n",
      "+---------------+--------+-------------------+\n",
      "|      <1H OCEAN|     0.0|      (5,[0],[1.0])|\n",
      "|         ISLAND|     4.0|      (5,[4],[1.0])|\n",
      "|         INLAND|     1.0|      (5,[1],[1.0])|\n",
      "|       NEAR BAY|     3.0|      (5,[3],[1.0])|\n",
      "|     NEAR OCEAN|     2.0|      (5,[2],[1.0])|\n",
      "+---------------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val encoder = new OneHotEncoder().setInputCol(\"op_index\").setOutputCol(\"ocean_proximity_out\").setDropLast(false)\n",
    "val encoded = encoder.transform(indexed)\n",
    "encoded.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- housing_median_age: float (nullable = true)\n",
      " |-- total_rooms: float (nullable = true)\n",
      " |-- total_bedrooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- median_income: float (nullable = true)\n",
      " |-- median_house_value: float (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      " |-- income_cat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_income_cat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline combined with Imputer, StringIndexer and OneHotEncoder to fit and transform then add additional 3 fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- housing_median_age: float (nullable = true)\n",
      " |-- total_rooms: float (nullable = true)\n",
      " |-- total_bedrooms: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- households: float (nullable = true)\n",
      " |-- median_income: float (nullable = true)\n",
      " |-- median_house_value: float (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      " |-- income_cat: double (nullable = true)\n",
      " |-- total_bedrooms_out: float (nullable = true)\n",
      " |-- op_index: double (nullable = true)\n",
      " |-- ocean_proximity_out: vector (nullable = true)\n",
      " |-- room_per_household: double (nullable = true)\n",
      " |-- population_per_household: double (nullable = true)\n",
      " |-- bedrooms_per_room: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(imputer, indexer, encoder))\n",
    "//we have to repeat exactly the same process to test set.  It's too much hassel\n",
    "val pipelineModel = pipeline.fit(housing_income_cat)\n",
    "val housing_temp = pipelineModel.transform(housing_income_cat).\n",
    "                withColumn(\"room_per_household\", $\"total_rooms\" / $\"households\").\n",
    "                withColumn(\"population_per_household\", $\"population\" / $\"households\").\n",
    "                withColumn(\"bedrooms_per_room\", $\"total_bedrooms_out\" / $\"total_rooms\")\n",
    "                           \n",
    "housing_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+------------------+--------+-------------------+------------------+------------------------+-------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|income_cat|total_bedrooms_out|op_index|ocean_proximity_out|room_per_household|population_per_household|bedrooms_per_room  |\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+------------------+--------+-------------------+------------------+------------------------+-------------------+\n",
      "|-122.16  |37.77   |47.0              |1256.0     |null          |570.0     |218.0     |4.375        |161900.0          |NEAR BAY       |3.0       |435.0             |3.0     |(5,[3],[1.0])      |5.761467889908257 |2.614678899082569       |0.3463375796178344 |\n",
      "|-122.17  |37.75   |38.0              |992.0      |null          |732.0     |259.0     |1.6196       |85100.0           |NEAR BAY       |2.0       |435.0             |3.0     |(5,[3],[1.0])      |3.83011583011583  |2.8262548262548264      |0.43850806451612906|\n",
      "|-122.28  |37.78   |29.0              |5154.0     |null          |3741.0    |1273.0    |2.5762       |173400.0          |NEAR BAY       |2.0       |435.0             |3.0     |(5,[3],[1.0])      |4.048703849175177 |2.93872741555381        |0.08440046565774156|\n",
      "|-122.24  |37.75   |45.0              |891.0      |null          |384.0     |146.0     |4.9489       |247100.0          |NEAR BAY       |4.0       |435.0             |3.0     |(5,[3],[1.0])      |6.102739726027397 |2.6301369863013697      |0.4882154882154882 |\n",
      "|-122.1   |37.69   |41.0              |746.0      |null          |387.0     |161.0     |3.9063       |178400.0          |NEAR BAY       |3.0       |435.0             |3.0     |(5,[3],[1.0])      |4.633540372670807 |2.4037267080745344      |0.5831099195710456 |\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----------+------------------+--------+-------------------+------------------+------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_temp.filter(isnull($\"total_bedrooms\")).show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for StandardScaler that only accept Vector as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use median_house_value as label.  Features covers all numeric fields.  OP is a vector for categorical data.   Income_cat is for spliting data in stratified way.   Manual manipulation only works for limited features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+\n",
      "|label   |features                                                                                                                                          |op           |income_cat|\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+\n",
      "|452600.0|[-122.2300033569336,37.880001068115234,41.0,880.0,129.0,322.0,126.0,8.325200080871582,6.984126984126984,2.5555555555555554,0.14659090909090908]   |(5,[3],[1.0])|5.0       |\n",
      "|358500.0|[-122.22000122070312,37.86000061035156,21.0,7099.0,1106.0,2401.0,1138.0,8.301400184631348,6.238137082601054,2.109841827768014,0.15579659106916466]|(5,[3],[1.0])|5.0       |\n",
      "|352100.0|[-122.23999786376953,37.849998474121094,52.0,1467.0,190.0,496.0,177.0,7.257400035858154,8.288135593220339,2.8022598870056497,0.12951601908657123] |(5,[3],[1.0])|5.0       |\n",
      "|341300.0|[-122.25,37.849998474121094,52.0,1274.0,235.0,558.0,219.0,5.643099784851074,5.8173515981735155,2.547945205479452,0.18445839874411302]             |(5,[3],[1.0])|4.0       |\n",
      "|342200.0|[-122.25,37.849998474121094,52.0,1627.0,280.0,565.0,259.0,3.8461999893188477,6.281853281853282,2.1814671814671813,0.1720958819913952]             |(5,[3],[1.0])|3.0       |\n",
      "+--------+--------------------------------------------------------------------------------------------------------------------------------------------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg._\n",
    "val housing_temp2 = housing_temp.rdd.map(r => (r.getFloat(8), Vectors.dense(\n",
    "    r.getFloat(0), r.getFloat(1), r.getFloat(2), r.getFloat(3), r.getFloat(11), r.getFloat(5), r.getFloat(6), \n",
    "    r.getFloat(7), r.getDouble(14), r.getDouble(15), r.getDouble(16)), r.getAs[Vector](13), r.getDouble(10))).\n",
    "    toDF(\"label\", \"features\", \"op\", \"income_cat\")                                        \n",
    "\n",
    "housing_temp2.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20640"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_temp2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply StandardScaler to 'features' field with mean = true to center around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------------+----------+--------------------+\n",
      "|   label|            features|           op|income_cat|      scaledFeatures|\n",
      "+--------+--------------------+-------------+----------+--------------------+\n",
      "|452600.0|[-122.23000335693...|(5,[3],[1.0])|       5.0|[-1.3278047216384...|\n",
      "|358500.0|[-122.22000122070...|(5,[3],[1.0])|       5.0|[-1.3228124691993...|\n",
      "|352100.0|[-122.23999786376...|(5,[3],[1.0])|       5.0|[-1.3327931661046...|\n",
      "|341300.0|[-122.25,37.84999...|(5,[3],[1.0])|       4.0|[-1.3377854185437...|\n",
      "|342200.0|[-122.25,37.84999...|(5,[3],[1.0])|       3.0|[-1.3377854185437...|\n",
      "+--------+--------------------+-------------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaledFeatures\").\n",
    "    setWithStd(true).setWithMean(true)\n",
    "val scalerModel = scaler.fit(housing_temp2)\n",
    "val scaledData = scalerModel.transform(housing_temp2)\n",
    "scaledData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine numeric vector and categorical vector into one feature vector to prepare for data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|label   |features                                                                                                                                                                                                                                        |income_cat|\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "|452600.0|[-1.3278047216384026,1.0525232947285934,0.9821188656747666,-0.804799599801809,-0.9724529206674336,-0.9744049915469923,-0.977009185045236,2.3447089981016496,0.6285442264151613,-0.049595334664572825,-1.0299628811021717,0.0,0.0,0.0,1.0,0.0]   |5.0       |\n",
      "|358500.0|[-1.3228124691993037,1.0431595757874317,-0.6070042082805048,2.0458405373571247,1.3571105530973857,0.8614179998296625,1.6699205725917976,2.33218156144263,0.32703343493534437,-0.09250998908805363,-0.8888756697443062,0.0,0.0,0.0,1.0,0.0]      |5.0       |\n",
      "|352100.0|[-1.3327931661046264,1.0384768233434631,1.8561365563501657,-0.5357329073251578,-0.8270042288561193,-0.8207574684591915,-0.8436164798678757,1.7826562356282514,1.155592470660819,-0.025841901757064786,-1.2916543691032398,0.0,0.0,0.0,1.0,0.0]  |5.0       |\n",
      "|341300.0|[-1.3377854185437252,1.0384768233434631,1.8561365563501657,-0.6241994689060669,-0.7197060135854777,-0.7660095004623889,-0.7337636638394612,0.9329447940740229,0.15696227955717026,-0.050328081993623826,-0.4496019117668218,0.0,0.0,0.0,1.0,0.0]|4.0       |\n",
      "|342200.0|[-1.3377854185437252,1.0384768233434631,1.8561365563501657,-0.4623927526466838,-0.612407798314836,-0.7598282782692015,-0.6291419342885903,-0.012880689529578507,0.344702448698221,-0.08561368522679019,-0.6390710915761776,0.0,0.0,0.0,1.0,0.0] |3.0       |\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val combinedData = scaledData.rdd.map(r => (r.getFloat(0), new DenseVector( \n",
    "     r.getAs[Vector](4).toArray ++ r.getAs[Vector](2).toArray), r.getDouble(3))).\n",
    "     toDF(\"label\", \"features\", \"income_cat\")\n",
    "combinedData.show(5, false)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16549"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val strat_train_temp = combinedData.stat.sampleBy(\"income_cat\", fractions, 42L)\n",
    "val strat_test_temp = spark.createDataFrame(combinedData.rdd.subtract(strat_train_temp.rdd), combinedData.schema)\n",
    "strat_train_temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label   |features                                                                                                                                                                                                                                           |\n",
      "+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|452600.0|[-1.3278047216384026,1.0525232947285934,0.9821188656747666,-0.804799599801809,-0.9724529206674336,-0.9744049915469923,-0.977009185045236,2.3447089981016496,0.6285442264151613,-0.049595334664572825,-1.0299628811021717,0.0,0.0,0.0,1.0,0.0]      |\n",
      "|342200.0|[-1.3377854185437252,1.0384768233434631,1.8561365563501657,-0.4623927526466838,-0.612407798314836,-0.7598282782692015,-0.6291419342885903,-0.012880689529578507,0.344702448698221,-0.08561368522679019,-0.6390710915761776,0.0,0.0,0.0,1.0,0.0]    |\n",
      "|269700.0|[-1.3377854185437252,1.0384768233434631,1.8561365563501657,-0.786922937098931,-0.7721629188289025,-0.8940491030355563,-0.8017677880475274,0.08744447073315052,-0.26972312505888124,-0.0896162473329281,0.27555666668349765,0.0,0.0,0.0,1.0,0.0]    |\n",
      "|299200.0|[-1.3377854185437252,1.0337958568462697,1.8561365563501657,-0.0461873748463442,-0.11406719850230053,-0.29270448681261163,0.03782159159821163,-0.1113635796953929,-0.20091283002127436,-0.0907227030384837,-0.320234147396495,0.0,0.0,0.0,1.0,0.0]  |\n",
      "|241400.0|[-1.3377854185437252,1.0337958568462697,1.8561365563501657,0.21462855022897875,0.35804494868852266,-0.23707348707392512,0.38568884235485734,-0.39512713757267826,-0.25522574965139055,-0.12347347989876004,0.11545489118078874,0.0,0.0,0.0,1.0,0.0]|\n",
      "+--------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val train_set = strat_train_temp.drop(\"income_cat\")\n",
    "val test_set = strat_test_temp.drop(\"income_cat\")\n",
    "train_set.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model Fit, Transform and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0, current: 0.001)\n",
      "featuresCol: features column name (default: features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 1.0E-9)\n",
      "solver: the solver algorithm for optimization. If this is not set or empty, default value is 'auto' (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression._\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "\n",
    "val lr =  new LinearRegression().setMaxIter(10)\n",
    "//Param for the ElasticNet mixing parameter, in range [0, 1]. \n",
    "//For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n",
    "//For regParam, 1 penalize model complexity and 0: training error accounts for all\n",
    "val pgLr = new ParamGridBuilder().\n",
    "    addGrid(lr.regParam, Array(0.001, 0.000001, 0.000000001)).\n",
    "    addGrid(lr.elasticNetParam, Array(0.01, 0.001, 0.0001)).\n",
    "    build()\n",
    "val evaluator = new RegressionEvaluator().setMetricName(\"rmse\").setLabelCol(\"label\").setPredictionCol(\"prediction\")\n",
    "\n",
    "val cvLr = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).\n",
    "  setEstimatorParamMaps(pgLr).setNumFolds(5).setSeed(42L)\n",
    "\n",
    "val lrModel = cvLr.fit(train_set)\n",
    "println(lrModel.bestModel.explainParams()+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 0.001,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-9\n",
       "},69597.44343297838), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 1.0E-4,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-9\n",
       "},69597.44343297838), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 0.01,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-9\n",
       "},69597.4434329784), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 1.0E-4,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-6\n",
       "},69597.4434330068), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 0.001,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-6\n",
       "},69597.4434330071), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 0.01,\n",
       "\tlinReg_92b756da9d32-regParam: 1.0E-6\n",
       "},69597.44343300952), ({\n",
       "\tlinReg_92b756da9d32-elasticNetParam: 1.0E-4,\n",
       "\tlinReg_92b756da9d32-regParam: 0.001..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lrModel.getEstimatorParamMaps zip lrModel.avgMetrics).sortBy(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69399.03027776726"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(lrModel.transform(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+\n",
      "|   label|            features|        prediction|\n",
      "+--------+--------------------+------------------+\n",
      "|117400.0|[0.68863496282266...|177420.43915487675|\n",
      "|320000.0|[-1.1630946629040...|287508.88277954864|\n",
      "|113900.0|[0.31928824975972...|140922.81431683444|\n",
      "| 83100.0|[-1.3527583678881...| 94289.23368225707|\n",
      "|187600.0|[0.19450859459237...|230204.37301735595|\n",
      "| 91300.0|[-0.9484735036972...|139929.35129356917|\n",
      "|110200.0|[0.65868906413382...|175891.94619083253|\n",
      "|308700.0|[-1.2529361669434...|  296036.337390525|\n",
      "|203800.0|[0.53390940896647...|170068.96368219485|\n",
      "|500001.0|[0.60378571122235...| 277227.8018432234|\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val lrPrediction = lrModel.transform(test_set)\n",
    "lrPrediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68279.11552227607"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(lrPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above is NOT a case of overfitting.  When a model is overfitting, the metric evaluated on test set is much worser than one on traing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression Model Fit, Transform and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false, current: true)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10)\n",
      "featuresCol: features column name (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 8)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256, current: 512)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1, current: 20)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: 926680331)\n",
      "varianceCol: Column name for the biased sample variance of prediction (undefined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dtr =  new DecisionTreeRegressor().setMaxMemoryInMB(512).setCacheNodeIds(true)\n",
    "val pgDtr = new ParamGridBuilder().\n",
    "    addGrid(dtr.maxBins, Array(8, 16, 32)).\n",
    "    addGrid(dtr.maxDepth, Array(2, 3, 5, 8)).\n",
    "    addGrid(dtr.minInstancesPerNode, Array(20, 50, 100)).\n",
    "    build()\n",
    "    \n",
    "val cvDtr = new CrossValidator().setEstimator(dtr).setEvaluator(evaluator).\n",
    "  setEstimatorParamMaps(pgDtr).setNumFolds(5).setSeed(42L) \n",
    "  \n",
    "val dtrModel = cvDtr.fit(train_set) \n",
    "println(dtrModel.bestModel.explainParams()+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(({\n",
       "\tdtr_5f09103936d4-maxBins: 32,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4-minInstancesPerNode: 20\n",
       "},61212.522915586065), ({\n",
       "\tdtr_5f09103936d4-maxBins: 16,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4-minInstancesPerNode: 20\n",
       "},61721.49217717871), ({\n",
       "\tdtr_5f09103936d4-maxBins: 32,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4-minInstancesPerNode: 50\n",
       "},61793.54800918797), ({\n",
       "\tdtr_5f09103936d4-maxBins: 16,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4-minInstancesPerNode: 50\n",
       "},62156.30331940999), ({\n",
       "\tdtr_5f09103936d4-maxBins: 32,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4-minInstancesPerNode: 100\n",
       "},62944.89901907229), ({\n",
       "\tdtr_5f09103936d4-maxBins: 16,\n",
       "\tdtr_5f09103936d4-maxDepth: 8,\n",
       "\tdtr_5f09103936d4..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dtrModel.getEstimatorParamMaps zip dtrModel.avgMetrics).sortBy(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56611.404566064"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(dtrModel.transform(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+\n",
      "|   label|            features|        prediction|\n",
      "+--------+--------------------+------------------+\n",
      "|117400.0|[0.68863496282266...|147415.38461538462|\n",
      "|320000.0|[-1.1630946629040...|         303134.68|\n",
      "|113900.0|[0.31928824975972...| 89885.20408163265|\n",
      "| 83100.0|[-1.3527583678881...| 86694.04761904762|\n",
      "|187600.0|[0.19450859459237...| 202633.4976076555|\n",
      "| 91300.0|[-0.9484735036972...|125391.00529100529|\n",
      "|110200.0|[0.65868906413382...|147415.38461538462|\n",
      "|308700.0|[-1.2529361669434...|235402.79695431472|\n",
      "|203800.0|[0.53390940896647...|191588.51174934726|\n",
      "|500001.0|[0.60378571122235...|          384011.5|\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val dtrPrediction = dtrModel.transform(test_set)\n",
    "dtrPrediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59475.62025137472"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(dtrPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above seems to a little overfitting. There are no regParm or elasticNet to tune. Research show it's common for decision tree models to be over-grown and overfitting.  Pruning can help alleviate overfitting.  I add minInstancesPerNode constraint.  The seems to reduce variance and overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression Model Fit, Transform and Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false, current: true)\n",
      "checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations (default: 10)\n",
      "featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)\n",
      "featuresCol: features column name (default: features)\n",
      "impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)\n",
      "labelCol: label column name (default: label)\n",
      "maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32, current: 32)\n",
      "maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. (default: 5, current: 16)\n",
      "maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256, current: 512)\n",
      "minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)\n",
      "minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1, current: 20)\n",
      "numTrees: Number of trees to train (>= 1) (default: 20)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "seed: random seed (default: 235498149)\n",
      "subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val rfr =  new RandomForestRegressor().setMaxMemoryInMB(512).setCacheNodeIds(true)\n",
    "\n",
    "val pgRfr = new ParamGridBuilder().\n",
    "    addGrid(rfr.maxBins, Array(8, 16, 32)).\n",
    "    addGrid(rfr.maxDepth, Array(5, 8, 12, 16)).\n",
    "    addGrid(rfr.minInstancesPerNode, Array(20)).\n",
    "    build()\n",
    "    \n",
    "val cvRfr = new CrossValidator().setEstimator(rfr).setEvaluator(evaluator).\n",
    "  setEstimatorParamMaps(pgRfr).setNumFolds(5).setSeed(42L) \n",
    "val rfrModel = cvRfr.fit(train_set) \n",
    "println(rfrModel.bestModel.explainParams()+\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(({\n",
       "\trfr_add3f40f2ffa-maxBins: 32,\n",
       "\trfr_add3f40f2ffa-maxDepth: 16,\n",
       "\trfr_add3f40f2ffa-minInstancesPerNode: 20\n",
       "},53995.93034139974), ({\n",
       "\trfr_add3f40f2ffa-maxBins: 32,\n",
       "\trfr_add3f40f2ffa-maxDepth: 12,\n",
       "\trfr_add3f40f2ffa-minInstancesPerNode: 20\n",
       "},54234.685943040604), ({\n",
       "\trfr_add3f40f2ffa-maxBins: 16,\n",
       "\trfr_add3f40f2ffa-maxDepth: 16,\n",
       "\trfr_add3f40f2ffa-minInstancesPerNode: 20\n",
       "},55517.5368158844), ({\n",
       "\trfr_add3f40f2ffa-maxBins: 16,\n",
       "\trfr_add3f40f2ffa-maxDepth: 12,\n",
       "\trfr_add3f40f2ffa-minInstancesPerNode: 20\n",
       "},55754.37328374521), ({\n",
       "\trfr_add3f40f2ffa-maxBins: 32,\n",
       "\trfr_add3f40f2ffa-maxDepth: 8,\n",
       "\trfr_add3f40f2ffa-minInstancesPerNode: 20\n",
       "},57540.076427766646), ({\n",
       "\trfr_add3f40f2ffa-maxBins: 16,\n",
       "\trfr_add3f40f2ffa-maxDepth: 8,\n",
       "\trfr_add3f40f2..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rfrModel.getEstimatorParamMaps zip rfrModel.avgMetrics).sortBy(_._2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47342.5567613887"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(rfrModel.transform(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------------+\n",
      "|   label|            features|        prediction|\n",
      "+--------+--------------------+------------------+\n",
      "|117400.0|[0.68863496282266...|150101.23952855787|\n",
      "|320000.0|[-1.1630946629040...|353019.82845839777|\n",
      "|113900.0|[0.31928824975972...|105523.13078073513|\n",
      "| 83100.0|[-1.3527583678881...| 90497.28232335419|\n",
      "|187600.0|[0.19450859459237...| 209087.4955315608|\n",
      "| 91300.0|[-0.9484735036972...|114915.31531508919|\n",
      "|110200.0|[0.65868906413382...|121627.91963113724|\n",
      "|308700.0|[-1.2529361669434...|235375.40572882886|\n",
      "|203800.0|[0.53390940896647...|175043.05214427217|\n",
      "|500001.0|[0.60378571122235...|392332.21383996226|\n",
      "+--------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val rfrPrediction = rfrModel.transform(test_set)\n",
    "rfrPrediction.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52057.414977450644"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(rfrPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above is still a little overfitting and it is better than before (10 numTree and no minInstancesPerNode, 30K+ difference in metrics value).  Increasing numTree help stablize the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.060573039498378244, 0.04765862892710125, 0.038085786043434794, 0.006481938131746838, 0.006393222902016197, 0.007132552001144698, 0.005379025230251163, 0.3089598383060173, 0.054836496090242694, 0.10789750310754889, 0.1065564500282175, 0.006052499578144016, 0.23744360585703922, 0.004638894234662272, 0.0019105200640549137, 0.0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val imp_features_arr = rfrModel.bestModel.asInstanceOf[RandomForestRegressionModel].featureImportances.toArray\n",
    "imp_features_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((median_income,0.3089598383060173), ([INLAND],0.23744360585703922), (population_per_household,0.10789750310754889), (bedrooms_per_room,0.1065564500282175), (longitude,0.060573039498378244), (room_per_household,0.054836496090242694), (latitude,0.04765862892710125), (housing_median_age,0.038085786043434794), (population,0.007132552001144698), (total_rooms,0.006481938131746838), (total_bedrooms,0.006393222902016197), ([<1H OCEAN],0.006052499578144016), (households,0.005379025230251163), ([NEAR OCEAN],0.004638894234662272), ([NEAR BAY],0.0019105200640549137), ([ISLAND],0.0))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fns = housing_temp.schema.fieldNames\n",
    "((fns.take(8) ++ fns.slice(14, 17) ++ op_cat_arr) zip imp_features_arr).sortBy(_._2)(Ordering[Double].reverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
